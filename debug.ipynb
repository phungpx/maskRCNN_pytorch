{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "pad_to_square = iaa.PadToSquare(position='right-bottom')\n",
    "\n",
    "def preprocess(image, image_size=(800, 800)):\n",
    "    sample = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    sample = pad_to_square(image=sample)\n",
    "    sample = cv2.resize(sample, dsize=image_size)\n",
    "    samples = torch.from_numpy(sample).unsqueeze(dim=0)\n",
    "    samples = samples.permute(0, 3, 1, 2).contiguous()\n",
    "    samples = samples.float().div(255.)\n",
    "    return samples\n",
    "\n",
    "def process(samples):\n",
    "    with torch.no_grad():\n",
    "        return model(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phungpx/anaconda3/envs/phungpx/lib/python3.7/site-packages/torchvision/ops/boxes.py:101: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  /opt/conda/conda-bld/pytorch_1595629427478/work/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  keep = keep.nonzero().squeeze(1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.337468385696411\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('/home/phungpx/Downloads/dog.jpg')\n",
    "samples = preprocess(image)\n",
    "t1 = time.time()\n",
    "preds = process(samples)\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_iou_nms = 0.5\n",
    "thresh_score = 0.5\n",
    "ratio_color_mask_on_image = 0.3\n",
    "\n",
    "for pred in preds:\n",
    "    labels, boxes, scores, masks = pred['labels'], pred['boxes'], pred['scores'], pred['masks']\n",
    "\n",
    "    indices = torchvision.ops.nms(boxes, scores, thresh_iou_nms)\n",
    "    labels, boxes, scores, masks = labels[indices], boxes[indices], scores[indices], masks[indices]\n",
    "\n",
    "    indices = scores > thresh_score\n",
    "    labels, boxes, scores, masks = labels[indices], boxes[indices], scores[indices], masks[indices]\n",
    "\n",
    "    labels = labels.detach().cpu().numpy()\n",
    "    boxes = boxes.detach().cpu().numpy()\n",
    "    scores = scores.detach().cpu().numpy()\n",
    "    masks = masks.round().to(torch.float).squeeze(dim=1).detach().cpu().numpy()\n",
    "\n",
    "    size = max(image.shape[0], image.shape[1])\n",
    "    rw, rh = size / masks.shape[2], size / masks.shape[1]\n",
    "    for (label, box, score, mask) in zip(labels, boxes, scores, masks):\n",
    "        mask = cv2.resize(mask, dsize=(size, size), interpolation=cv2.INTER_NEAREST)\n",
    "        mask = mask[:image.shape[0], :image.shape[1]]\n",
    "        box = np.int32([box[0] * rw, box[1] * rh, box[2] * rw, box[3] * rh])\n",
    "        image = visualize_bbox(image=image, bbox=box, class_name='dog')\n",
    "        image[mask.astype(dtype=bool)] = image[mask.astype(dtype=bool)] * (1 - ratio_color_mask_on_image) \\\n",
    "                                         + np.array([[0, 255, 0]], dtype=float) * ratio_color_mask_on_image\n",
    "\n",
    "    cv2.imshow('image', image)\n",
    "    cv2.waitKey()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bbox(image, bbox, mask, class_name, bbox_color=(255, 0, 0), mask_color=(0, 255, 0), text_color=(255, 255, 255), thickness=2):\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    cv2.rectangle(img=image,\n",
    "                  pt1=(int(x1), int(y1)),\n",
    "                  pt2=(int(x2), int(y2)),\n",
    "                  color=bbox_color,\n",
    "                  thickness=thickness)\n",
    "    ((text_width, text_height), _) = cv2.getTextSize(class_name, cv2.FONT_HERSHEY_SIMPLEX, 0.35, 1)\n",
    "    cv2.rectangle(img=image,\n",
    "                  pt1=(int(x1), int(y1 - 1.3 * text_height)),\n",
    "                  pt2=(int(x1 + text_width), int(y1)),\n",
    "                  color=bbox_color,\n",
    "                  thickness=-1)\n",
    "    cv2.putText(img=image,\n",
    "                text=class_name,\n",
    "                org=(int(x1), int(y1 - 0.3 * text_height)),\n",
    "                fontFace=cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                fontScale=0.35,\n",
    "                color=text_color,\n",
    "                lineType=cv2.LINE_AA)\n",
    "    image[mask.astype(dtype=bool)] = image[mask.astype(dtype=bool)] * (1 - ratio_color_mask_on_image) \\\n",
    "                                     + np.array([mask_color], dtype=float) * ratio_color_mask_on_image\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padd To Square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "def pad_to_square(image, value=0):\n",
    "    height, width = image.shape[:2]\n",
    "    if height > width:\n",
    "        pad = np.empty(shape=(height, height - width, 3), dtype=image.dtype)\n",
    "        pad.fill(value)\n",
    "        padded_image = np.concatenate([image, pad], axis=1)\n",
    "    elif width > height:\n",
    "        pad = np.empty(shape=(width - height, width, 3), dtype=image.dtype)\n",
    "        pad.fill(value)\n",
    "        padded_image = np.concatenate([image, pad], axis=0)\n",
    "    else:\n",
    "        padded_image = image\n",
    "    return padded_image\n",
    "\n",
    "pad_to_square_iaa = iaa.PadToSquare(position='right-bottom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(602, 1200, 3)\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('/home/phungpx/Downloads/dog.jpg')\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002534151077270508\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "padded_image = pad_to_square(image=image)\n",
    "t2 = time.time()\n",
    "print(t2 - t1)\n",
    "\n",
    "cv2.imshow('padding image', padded_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010089397430419922\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "iaa_padded_image = pad_to_square_iaa(image=image)\n",
    "t2 = time.time()\n",
    "print(t2 - t1)\n",
    "\n",
    "cv2.imshow('padding image', iaa_padded_image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compare results\n",
    "(padded_image - iaa_padded_image).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = 'dataset/PASCAL_VOC/images/'\n",
    "label_dir = 'dataset/PASCAL_VOC/labels/'\n",
    "csv_path = 'dataset/PASCAL_VOC/train.csv'\n",
    "image_size = 416\n",
    "S = [13, 26, 52]  # image_size // 32, image_size // 16, image_size // 8\n",
    "C = 20\n",
    "anchors = [[[0.28, 0.22], [0.38, 0.48], [0.9, 0.78]],\n",
    "           [[0.07, 0.15], [0.15, 0.11], [0.14, 0.29]],\n",
    "           [[0.02, 0.03], [0.04, 0.07], [0.08, 0.06]]]\n",
    "transforms = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.data.voc_dataset import YOLOv3Dataset\n",
    "\n",
    "dataset = YOLOv3Dataset(image_dir, label_dir, csv_path, image_size, anchors, S, C, transforms=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataloader = DataLoader(dataset=dataset, batch_size=3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "def convert_cxcywh2xyxy(bboxes: torch.Tensor, image_width: int, image_height: int) -> torch.Tensor:\n",
    "    '''convert (center_x / image_width, center_y / image_height, w / image_width, h / image_height) to (x1, y1, x2, y2)\n",
    "    '''\n",
    "    bboxes[:, [2, 4]] = bboxes[:, [2, 4]] * image_width\n",
    "    bboxes[:, [3, 5]] = bboxes[:, [3, 5]] * image_height\n",
    "    bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 4] / 2\n",
    "    bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 5] / 2\n",
    "    bboxes[:, 4] = bboxes[:, 2] + bboxes[:, 4]\n",
    "    bboxes[:, 5] = bboxes[:, 3] + bboxes[:, 5]\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "def cells_to_bboxes(bboxes: torch.Tensor, scale_anchors: torch.Tensor,\n",
    "                    S: int, is_prediction: bool = True) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Scales the predictions coming from the model to be relative to the entire image.\n",
    "    Args:\n",
    "        bboxes (Tensor[N, 3, S, S, num_classes + 5])\n",
    "        anchors: the anchors used for the predictions\n",
    "        S: the number of cells the image is divided in on the width (and height)\n",
    "        is_preds: whether the input is predictions or the true bounding boxes\n",
    "    Returns:\n",
    "        converted_bboxes: the converted boxes of sizes (N, num_anchors * S * S, 1+5)\n",
    "                          with class index, object score, bounding box coordinates.\n",
    "    \"\"\"\n",
    "    BATCH_SIZE = bboxes.shape[0]\n",
    "    num_anchors = scale_anchors.shape[0]\n",
    "    box_coords = bboxes[..., 1:5]\n",
    "    if is_prediction:\n",
    "        scale_anchors = scale_anchors.reshape(1, num_anchors, 1, 1, 2)\n",
    "        box_coords[..., 0:2] = torch.sigmoid(box_coords[..., 0:2])  # x, y coordinates\n",
    "        box_coords[..., 2:4] = torch.exp(box_coords[..., 2:4]) * scale_anchors  # w, h\n",
    "        scores = torch.sigmoid(bboxes[..., 0:1])\n",
    "        labels = torch.argmax(bboxes[..., 5:], dim=-1).unsqueeze(dim=-1)\n",
    "    else:\n",
    "        scores = bboxes[..., 0:1]\n",
    "        labels = bboxes[..., 5:6]\n",
    "\n",
    "    # BATCH_SIZE x 3 x S x S x 1\n",
    "    cell_indices = torch.arange(S).repeat(BATCH_SIZE, 3, S, 1).unsqueeze(dim=-1).to(bboxes.device)\n",
    "\n",
    "    x = 1 / S * (box_coords[..., 0:1] + cell_indices)\n",
    "    y = 1 / S * (box_coords[..., 1:2] + cell_indices.permute(0, 1, 3, 2, 4))\n",
    "    w = 1 / S * box_coords[..., 2:3]\n",
    "    h = 1 / S * box_coords[..., 3:4]\n",
    "\n",
    "    bboxes = torch.cat([labels, scores, x, y, w, h], dim=-1).reshape(BATCH_SIZE, num_anchors * S * S, 6)\n",
    "\n",
    "    return bboxes\n",
    "\n",
    "def batched_nms(boxes: torch.Tensor, scores: torch.Tensor,\n",
    "                idxs: torch.Tensor, iou_threshold: float) -> torch.Tensor:\n",
    "    \"\"\"Performs non-maximum suppression in a batched fashion.\n",
    "    Each index value correspond to a category, and NMS will not be applied between elements of different categories.\n",
    "    Args:\n",
    "        boxes (Tensor[N, 4]): boxes where NMS will be performed. (x1, y1, x2, y2) with 0 <= x1 < x2 and 0 <= y1 < y2\n",
    "        scores (Tensor[N]): scores for each one of the boxes\n",
    "        idxs (Tensor[N]): indices of the categories for each one of the boxes.\n",
    "        iou_threshold (float): discards all overlapping boxes with IoU > iou_threshold\n",
    "    Returns:\n",
    "        keep (Tensor): int64 tensor with the indices of the elements that have been kept by NMS, sorted in decreasing order of scores\n",
    "    \"\"\"\n",
    "    if boxes.numel() == 0:\n",
    "        return torch.empty(size=(0,), dtype=torch.int64, device=boxes.device)\n",
    "    else:\n",
    "        max_coordinate = boxes.max()\n",
    "        offsets = idxs.to(boxes) * (max_coordinate + torch.tensor(data=1).to(boxes))\n",
    "        boxes_for_nms = boxes + offsets[:, None]\n",
    "        keep = torchvision.ops.nms(boxes=boxes_for_nms, scores=scores, iou_threshold=iou_threshold)\n",
    "        return keep\n",
    "\n",
    "def postprocess_batched_nms(bboxes: torch.Tensor, iou_threshold: float, score_threshold: float) -> torch.Tensor:\n",
    "    '''\n",
    "    Args:\n",
    "        bboxes (Tensor[N, 6]): (class_id, score, x1, y1, x2, y2)\n",
    "        iou_threshold (float): discards all overlapping boxes with IoU > iou_threshold\n",
    "        score_threshold (float): discards all boxes with score < score_threshold\n",
    "    Returns:\n",
    "        bboxes (Tensor[M, 6]): (class_id, score, x1, y1, x2, y2)\n",
    "    '''\n",
    "    # bboxes = torch.from_numpy(np.asarray(bboxes))\n",
    "    idxs, scores, boxes = bboxes[:, 0], bboxes[:, 1], bboxes[:, 2:6]\n",
    "    indices = scores >= score_threshold\n",
    "    idxs, scores, boxes = idxs[indices], scores[indices], boxes[indices]\n",
    "    indices = batched_nms(boxes=boxes, scores=scores, idxs=idxs, iou_threshold=iou_threshold)\n",
    "    idxs, scores, boxes = idxs[indices], scores[indices], boxes[indices]\n",
    "    bboxes = torch.cat([idxs[:, None], scores[:, None], boxes], dim=-1)\n",
    "\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "PASCAL_CLASSES = {\"aeroplane\": 0, \"bicycle\": 1, \"bird\": 2, \"boat\": 3, \"bottle\": 4,\n",
    "                  \"bus\": 5, \"car\": 6, \"cat\": 7, \"chair\": 8, \"cow\": 9,\n",
    "                  \"diningtable\": 10, \"dog\": 11, \"horse\": 12, \"motorbike\": 13, \"person\": 14,\n",
    "                  \"pottedplant\": 15, \"sheep\": 16, \"sofa\": 17, \"train\": 18, \"tvmonitor\": 19}\n",
    "\n",
    "def visualize_targets(batch, multi_scale_anchors, scales=[13, 26, 52], iou_threshold=0, score_threshold=1., classes=PASCAL_CLASSES):\n",
    "    '''\n",
    "    parameters:\n",
    "        batch_samples: Tensor [batch_size x 3 x image_size x image_size]\n",
    "        tuple_batch_targets: tuple([batch_size x num_anchors x S1 x S1 x 6],\n",
    "                        [batch_size x num_anchors x S2 x S2 x 6],\n",
    "                        [batch_size x num_anchors x S3 x S3 x 6])\n",
    "        multi_scale_anchors: 3 x 3 x 2\n",
    "        scales: [13, 26, 52]\n",
    "    '''\n",
    "    idx2class = {idx: label_name for label_name, idx in classes.items()}\n",
    "    multi_scale_anchors = torch.tensor(multi_scale_anchors)\n",
    "    scales = torch.tensor(scales).unsqueeze(dim=1).unsqueeze(dim=1).repeat(1, 3, 2)\n",
    "    multi_scale_anchors = multi_scale_anchors * scales\n",
    "\n",
    "    batch_samples, tuple_batch_targets, batch_infos = batch\n",
    "    batch_image_paths = batch_infos[0]\n",
    "    batch_samples = batch_samples.permute(0, 2, 3, 1).contiguous()\n",
    "    batch_samples = batch_samples.mul(255).to(torch.uint8)\n",
    "    batch_images = batch_samples.data.detach().cpu().numpy()\n",
    "    \n",
    "    batch_multi_scale_boxes = []\n",
    "    for scale_idx, batch_scale_targets in enumerate(tuple_batch_targets):\n",
    "        chosen_scale_anchors = multi_scale_anchors[scale_idx]\n",
    "        scale = batch_scale_targets.shape[2]\n",
    "        batch_scale_boxes = cells_to_bboxes(batch_scale_targets, is_prediction=False, S=scale, scale_anchors=chosen_scale_anchors)\n",
    "        batch_multi_scale_boxes.append(batch_scale_boxes)\n",
    "    \n",
    "    batch_multi_scale_boxes = torch.cat(batch_multi_scale_boxes, dim=1)\n",
    "\n",
    "    for image, multi_scale_bboxes, image_path in zip(batch_images, batch_multi_scale_boxes, batch_image_paths):\n",
    "        print('\\n')\n",
    "        print(f'image: {Path(image_path).name}')\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # convert RGB to BGR\n",
    "        image_height, image_width = image.shape[:2]\n",
    "        bboxes = convert_cxcywh2xyxy(bboxes=multi_scale_bboxes, image_width=image_width, image_height=image_height)\n",
    "        # num_bboxes: (S1 * S1 + S2 * S2 + S3 * S3) * num_anchor_per_scale\n",
    "        print(f'number of bboxes before NMS: {bboxes.shape[0]}')\n",
    "        bboxes = postprocess_batched_nms(bboxes=bboxes, iou_threshold=iou_threshold, score_threshold=score_threshold)\n",
    "        print(f'number of bboxes after NMS: {bboxes.shape[0]}')\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            label, score, [x1, y1, x2, y2] = bbox[0].item(), bbox[1].item(), bbox[2:].to(torch.int32).data.numpy().tolist()\n",
    "            print(x1, y1, x2, y2)\n",
    "            cv2.rectangle(img=image, pt1=(x1, y1), pt2=(x2, y2), color=(0, 0, 255), thickness=2)\n",
    "            print(f'class name: {idx2class[label]}, score: {score:.3f}, coords: x1={x1}, y1={y1}, x2={x2}, y2={y2}')\n",
    "            cv2.imshow(Path(image_path).name, image)\n",
    "            cv2.waitKey()\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples:  torch.Size([3, 3, 416, 416])\n",
      "target 1:  torch.Size([3, 3, 13, 13, 6])\n",
      "target 2:  torch.Size([3, 3, 26, 26, 6])\n",
      "target 3:  torch.Size([3, 3, 52, 52, 6])\n",
      "path infos:  dataset/PASCAL_VOC/images/000007.jpg\n",
      "width infos:  tensor(500)\n",
      "height infos:  tensor(333)\n"
     ]
    }
   ],
   "source": [
    "batch_samples, tuple_batch_targets, batch_infos = data_iter.next()\n",
    "print('samples: ', batch_samples.shape)\n",
    "print('target 1: ', tuple_batch_targets[0].shape)\n",
    "print('target 2: ', tuple_batch_targets[1].shape)\n",
    "print('target 3: ', tuple_batch_targets[2].shape)\n",
    "print('path infos: ', batch_infos[0][0])\n",
    "print('width infos: ', batch_infos[1][0][0])\n",
    "print('height infos: ', batch_infos[1][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dataset/PASCAL_VOC/images/000007.jpg',\n",
       "  'dataset/PASCAL_VOC/images/000009.jpg',\n",
       "  'dataset/PASCAL_VOC/images/000016.jpg'),\n",
       " [tensor([500, 500, 334]), tensor([333, 375, 500])]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "image: 000019.jpg\n",
      "number of bboxes before NMS: 10647\n",
      "number of bboxes after NMS: 1\n",
      "8 93 220 214\n",
      "class name: cat, score: 1.000, coords: x1=8, y1=93, x2=220, y2=214\n",
      "\n",
      "\n",
      "image: 000020.jpg\n",
      "number of bboxes before NMS: 10647\n",
      "number of bboxes after NMS: 1\n",
      "26 122 307 345\n",
      "class name: car, score: 1.000, coords: x1=26, y1=122, x2=307, y2=345\n",
      "\n",
      "\n",
      "image: 000021.jpg\n",
      "number of bboxes before NMS: 10647\n",
      "number of bboxes after NMS: 3\n",
      "0 194 150 321\n",
      "class name: dog, score: 1.000, coords: x1=0, y1=194, x2=150, y2=321\n",
      "173 29 278 400\n",
      "class name: person, score: 1.000, coords: x1=173, y1=29, x2=278, y2=400\n",
      "8 149 117 347\n",
      "class name: person, score: 1.000, coords: x1=8, y1=149, x2=117, y2=347\n"
     ]
    }
   ],
   "source": [
    "visualize_targets(batch=data_iter.next(), multi_scale_anchors=anchors, scales=S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "from pretrained.yolov3 import YOLOv3\n",
    "\n",
    "weight_path = './pretrained/78.1map_0.2threshold_PASCAL.tar'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = YOLOv3(in_channels=3, num_classes=20)\n",
    "model.load_state_dict(state_dict=torch.load(f=weight_path, map_location='cpu')['state_dict'])\n",
    "model.to(device)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = ['./dataset/PASCAL_VOC/images/000002.jpg', './dataset/PASCAL_VOC/images/000001.jpg']\n",
    "image_size = (416, 416)\n",
    "\n",
    "def preprocess(image_paths, image_size=(416, 416)):\n",
    "    images = [cv2.imread(image_path) for image_path in image_paths]\n",
    "    samples = [cv2.cvtColor(image, cv2.COLOR_BGR2RGB) for image in images]\n",
    "    samples = [cv2.resize(sample, dsize=image_size) for sample in samples]\n",
    "    samples = torch.from_numpy(np.array(samples))\n",
    "    samples = samples.permute(0, 3, 1, 2).contiguous()\n",
    "    samples = samples.float().div(255.)\n",
    "\n",
    "    return images, samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting time: 0.89522\n"
     ]
    }
   ],
   "source": [
    "images, samples = preprocess(image_paths=image_paths, image_size=image_size)\n",
    "t1 = time.time()\n",
    "predictions = model(samples)\n",
    "t2 = time.time()\n",
    "print(f'predicting time: {t2 - t1:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction at scale = 13:  torch.Size([2, 3, 13, 13, 25])\n",
      "prediction at scale = 26:  torch.Size([2, 3, 26, 26, 25])\n",
      "prediction at scale = 52:  torch.Size([2, 3, 52, 52, 25])\n"
     ]
    }
   ],
   "source": [
    "print('prediction at scale = 13: ', predictions[0].shape)\n",
    "print('prediction at scale = 26: ', predictions[1].shape)\n",
    "print('prediction at scale = 52: ', predictions[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PASCAL_CLASSES = {\"aeroplane\": 0, \"bicycle\": 1, \"bird\": 2, \"boat\": 3, \"bottle\": 4,\n",
    "                  \"bus\": 5, \"car\": 6, \"cat\": 7, \"chair\": 8, \"cow\": 9,\n",
    "                  \"diningtable\": 10, \"dog\": 11, \"horse\": 12, \"motorbike\": 13, \"person\": 14,\n",
    "                  \"pottedplant\": 15, \"sheep\": 16, \"sofa\": 17, \"train\": 18, \"tvmonitor\": 19}\n",
    "\n",
    "scales = [13, 26, 52]  # image_size // 32, image_size // 16, image_size // 8\n",
    "multi_scale_anchors = [[[0.28, 0.22], [0.38, 0.48], [0.9, 0.78]],\n",
    "                       [[0.07, 0.15], [0.15, 0.11], [0.14, 0.29]],\n",
    "                       [[0.02, 0.03], [0.04, 0.07], [0.08, 0.06]]]\n",
    "\n",
    "def module_time(module, module_name, *args):\n",
    "    start = time.time()\n",
    "    output = module(*args)\n",
    "    stop = time.time()\n",
    "    print(f'{module_name}: {stop - start:.4f}s')\n",
    "    return output\n",
    "\n",
    "def visualize_predictions(image_paths,\n",
    "                          multi_scale_anchors, scales=[13, 26, 52],\n",
    "                          iou_threshold=0.5, score_threshold=0.7, classes=PASCAL_CLASSES):\n",
    "    '''\n",
    "    Args:\n",
    "        batch_samples: Tensor [batch_size x 3 x image_size x image_size]\n",
    "        tuple_batch_targets: tuple([batch_size x num_anchors x S1 x S1 x 6],\n",
    "                        [batch_size x num_anchors x S2 x S2 x 6],\n",
    "                        [batch_size x num_anchors x S3 x S3 x 6])\n",
    "        multi_scale_anchors: 3 x 3 x 2\n",
    "        scales: [13, 26, 52]\n",
    "    Returns:\n",
    "        visual image\n",
    "    '''\n",
    "    idx2class = {idx: label_name for label_name, idx in classes.items()}\n",
    "    multi_scale_anchors = torch.tensor(multi_scale_anchors)\n",
    "    scales = torch.tensor(scales).unsqueeze(dim=1).unsqueeze(dim=1).repeat(1, 3, 2)\n",
    "    multi_scale_anchors = multi_scale_anchors * scales\n",
    "\n",
    "    images, samples = preprocess(image_paths)\n",
    "    multi_scale_predictions = module_time(model, 'YOLOv3', samples)\n",
    "\n",
    "    batch_multi_scale_bboxes = []\n",
    "    for scale_idx, predictions in enumerate(multi_scale_predictions):\n",
    "        chosen_scale_anchors = multi_scale_anchors[scale_idx]\n",
    "        scale = predictions.shape[2]\n",
    "        # [batch_size, num_anchor_per_scale, scale, scale, 5 + num_classes]\n",
    "        batch_boxes = cells_to_bboxes(predictions, is_prediction=True, S=scale, scale_anchors=chosen_scale_anchors)\n",
    "        batch_multi_scale_bboxes.append(batch_boxes)\n",
    "\n",
    "    batch_multi_scale_bboxes = torch.cat(batch_multi_scale_bboxes, dim=1)\n",
    "\n",
    "    for image, multi_scale_bboxes, image_path in zip(images, batch_multi_scale_bboxes, image_paths):\n",
    "        print('\\n')\n",
    "        print(f'image: {Path(image_path).name}')\n",
    "\n",
    "        image_height, image_width = image.shape[:2]\n",
    "        bboxes = convert_cxcywh2xyxy(bboxes=multi_scale_bboxes, image_width=image_width, image_height=image_height)\n",
    "\n",
    "        # num_bboxes: (S1 * S1 + S2 * S2 + S3 * S3) * num_anchor_per_scale\n",
    "        print(f'number of bboxes before NMS: {bboxes.shape[0]}')\n",
    "        bboxes = postprocess_batched_nms(bboxes=bboxes, iou_threshold=iou_threshold, score_threshold=score_threshold)\n",
    "        print(f'number of bboxes after NMS: {bboxes.shape[0]}')\n",
    "\n",
    "        for bbox in bboxes:\n",
    "            label, score, [x1, y1, x2, y2] = bbox[0].item(), bbox[1].item(), bbox[2:].to(torch.int32).data.numpy().tolist()\n",
    "            cv2.rectangle(img=image, pt1=(x1, y1), pt2=(x2, y2), color=(0, 0, 255), thickness=2)\n",
    "            print(f'class name: {idx2class[label]}, score: {score:.3f}, coords: x1={x1}, y1={y1}, x2={x2}, y2={y2}')\n",
    "            cv2.imshow(Path(image_path).name, image)\n",
    "            cv2.waitKey()\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLOv3: 1.4657s\n",
      "\n",
      "\n",
      "image: 000017.jpg\n",
      "number of bboxes before NMS: 10647\n",
      "number of bboxes after NMS: 2\n",
      "class name: horse, score: 0.885, coords: x1=78, y1=64, x2=418, y2=343\n",
      "class name: person, score: 0.865, coords: x1=163, y1=53, x2=306, y2=204\n",
      "\n",
      "\n",
      "image: 000001.jpg\n",
      "number of bboxes before NMS: 10647\n",
      "number of bboxes after NMS: 2\n",
      "class name: person, score: 0.930, coords: x1=13, y1=2, x2=345, y2=495\n",
      "class name: dog, score: 0.788, coords: x1=49, y1=222, x2=186, y2=383\n",
      "\n",
      "\n",
      "image: 000004.jpg\n",
      "number of bboxes before NMS: 10647\n",
      "number of bboxes after NMS: 8\n",
      "class name: car, score: 0.893, coords: x1=12, y1=308, x2=86, y2=362\n",
      "class name: car, score: 0.890, coords: x1=359, y1=323, x2=497, y2=390\n",
      "class name: car, score: 0.877, coords: x1=134, y1=320, x2=189, y2=358\n",
      "class name: car, score: 0.869, coords: x1=229, y1=326, x2=334, y2=371\n",
      "class name: car, score: 0.864, coords: x1=107, y1=321, x2=148, y2=353\n",
      "class name: car, score: 0.839, coords: x1=0, y1=321, x2=21, y2=342\n",
      "class name: car, score: 0.835, coords: x1=163, y1=322, x2=252, y2=366\n",
      "class name: car, score: 0.816, coords: x1=82, y1=322, x2=114, y2=351\n"
     ]
    }
   ],
   "source": [
    "visualize_predictions(image_paths=['./dataset/PASCAL_VOC/images/000017.jpg',\n",
    "                                   './dataset/PASCAL_VOC/images/000001.jpg',\n",
    "                                   './dataset/PASCAL_VOC/images/000004.jpg'],\n",
    "                     multi_scale_anchors=multi_scale_anchors,\n",
    "                     scales=scales, classes=PASCAL_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.model.darknet53 import YOLOv3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = YOLOv3(in_channels=3, num_classes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict 1:  torch.Size([4, 3, 13, 13, 25])\n",
      "predict 2:  torch.Size([4, 3, 26, 26, 25])\n",
      "predict 3:  torch.Size([4, 3, 52, 52, 25])\n"
     ]
    }
   ],
   "source": [
    "print('predict 1: ', predictions[0].shape)\n",
    "print('predict 2: ', predictions[1].shape)\n",
    "print('predict 3: ', predictions[2].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.loss.yolov3_loss import YOLOv3Loss\n",
    "\n",
    "loss_fn = YOLOv3Loss(lambda_obj=1, lambda_noobj=10, lambda_bbox=10, lambda_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40.5704)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "preds = predictions[0].data.clone()\n",
    "trues = targets[0].data.clone()\n",
    "_anchors = torch.tensor(anchors[0]).data.clone()\n",
    "\n",
    "print(loss_fn(predictions=preds, targets=trues, anchors=_anchors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torchvision 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.36093\n"
     ]
    }
   ],
   "source": [
    "x = torch.FloatTensor(size=(1, 3, 800, 800))\n",
    "model.eval()\n",
    "t1 = time.time()\n",
    "a = model(x)\n",
    "t2 = time.time()\n",
    "print(round(t2 - t1, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = model.backbone\n",
    "rpn_anchor_generator = model.rpn.anchor_generator\n",
    "rpn_head = model.rpn.head\n",
    "box_roi_pool = model.roi_heads.box_roi_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_roi_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torchvision.models.detection.MaskRCNN(backbone=backbone,\n",
    "                                             num_classes=24,\n",
    "                                             rpn_anchor_generator=rpn_anchor_generator,\n",
    "                                             box_roi_pool=box_roi_pool,\n",
    "                                             min_size=320,\n",
    "                                             max_size=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaskRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(320,), max_size=640, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (0): ConvBNActivation(\n",
       "        (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "      (1): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
       "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(16, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
       "            (1): FrozenBatchNorm2d(64, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): ConvBNActivation(\n",
       "            (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): ConvBNActivation(\n",
       "            (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(24, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (4): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
       "            (1): FrozenBatchNorm2d(72, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ConvBNActivation(\n",
       "            (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (5): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ConvBNActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (6): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
       "            (1): FrozenBatchNorm2d(120, eps=1e-05)\n",
       "            (2): ReLU(inplace=True)\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ConvBNActivation(\n",
       "            (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(40, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (7): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
       "            (1): FrozenBatchNorm2d(240, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): ConvBNActivation(\n",
       "            (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (8): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "            (1): FrozenBatchNorm2d(200, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): ConvBNActivation(\n",
       "            (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (9): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): ConvBNActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (10): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
       "            (1): FrozenBatchNorm2d(184, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): ConvBNActivation(\n",
       "            (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(80, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (11): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
       "            (1): FrozenBatchNorm2d(480, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ConvBNActivation(\n",
       "            (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (12): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ConvBNActivation(\n",
       "            (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(112, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (13): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
       "            (1): FrozenBatchNorm2d(672, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ConvBNActivation(\n",
       "            (0): Conv2d(672, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (14): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ConvBNActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (15): InvertedResidual(\n",
       "        (block): Sequential(\n",
       "          (0): ConvBNActivation(\n",
       "            (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (1): ConvBNActivation(\n",
       "            (0): Conv2d(960, 960, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=960, bias=False)\n",
       "            (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "            (2): Hardswish()\n",
       "          )\n",
       "          (2): SqueezeExcitation(\n",
       "            (fc1): Conv2d(960, 240, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (fc2): Conv2d(240, 960, kernel_size=(1, 1), stride=(1, 1))\n",
       "          )\n",
       "          (3): ConvBNActivation(\n",
       "            (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(160, eps=1e-05)\n",
       "            (2): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (16): ConvBNActivation(\n",
       "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): FrozenBatchNorm2d(960, eps=1e-05)\n",
       "        (2): Hardswish()\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2d(160, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (1): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (cls_logits): Conv2d(256, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=24, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=96, bias=True)\n",
       "    )\n",
       "    (mask_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(14, 14), sampling_ratio=2)\n",
       "    (mask_head): MaskRCNNHeads(\n",
       "      (mask_fcn1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu1): ReLU(inplace=True)\n",
       "      (mask_fcn2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu2): ReLU(inplace=True)\n",
       "      (mask_fcn3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu3): ReLU(inplace=True)\n",
       "      (mask_fcn4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (relu4): ReLU(inplace=True)\n",
       "    )\n",
       "    (mask_predictor): MaskRCNNPredictor(\n",
       "      (conv5_mask): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (mask_fcn_logits): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskrcnnMobileNetV3(nn.Module):\n",
    "    def __init__(self, num_classes, pretrained=False, pretrained_backbone=False):\n",
    "        super(MaskrcnnMobileNetV3, self).__init__()\n",
    "        self.model = self._maskrcnn_mobilenet_v3(num_classes, pretrained, pretrained_backbone)\n",
    "\n",
    "    def _maskrcnn_mobilenet_v3(self, num_classes, pretrained, pretrained_backbone):\n",
    "        fasterrcnn_mobilenetv3 = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=pretrained,\n",
    "                                                                                                    pretrained_backbone=pretrained_backbone)\n",
    "        mobilenet_v3 = fasterrcnn_mobilenetv3.backbone\n",
    "        rpn_anchor_generator = fasterrcnn_mobilenetv3.rpn.anchor_generator\n",
    "        box_roi_pool = fasterrcnn_mobilenetv3.roi_heads.box_roi_pool\n",
    "\n",
    "        model = torchvision.models.detection.MaskRCNN(backbone=mobilenet_v3,\n",
    "                                                      num_classes=num_classes,\n",
    "                                                      rpn_anchor_generator=rpn_anchor_generator,\n",
    "                                                      box_roi_pool=box_roi_pool,\n",
    "                                                      mask_roi_pool=box_roi_pool)\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        return self.model(x, targets)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MaskrcnnMobileNetV3(num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6656076908111572\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "x = torch.FloatTensor(size=(1, 3, 800, 800))\n",
    "model.eval()\n",
    "t1 = time.time()\n",
    "model(x)\n",
    "t2 = time.time()\n",
    "print(t2 - t1)\n",
    "# outputs = backbone(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 25, 25])\n",
      "torch.Size([1, 256, 25, 25])\n",
      "torch.Size([1, 256, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "print(outputs['0'].shape)\n",
    "print(outputs['1'].shape)\n",
    "print(outputs['pool'].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
