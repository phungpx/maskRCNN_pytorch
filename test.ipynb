{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b5be5b-8c63-4bc4-a0c9-39f1c0e77d4e",
   "metadata": {},
   "source": [
    "# MaskRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f4d0b839-7e70-473a-b787-b63cbc556999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "from flame.core.model.maskRCNN.mask_rcnn import maskrcnn_resnet50_fpn, MaskRCNN_ResNet50_FPN_Weights\n",
    "from flame.core.model.maskRCNN.mask_rcnn import maskrcnn_resnet50_fpn_v2, MaskRCNN_ResNet50_FPN_V2_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c862e95b-c0a5-4f26-89a3-052f6f79eadd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 46357361\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = maskrcnn_resnet50_fpn_v2(\n",
    "    weights=None,\n",
    "    box_score_thresh=0.5,\n",
    "    box_nms_thresh=0.5\n",
    ")\n",
    "model.eval().to(device)\n",
    "print(f'Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abb39979-4b38-4986-95ea-50832cad09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('/home/phungpx/Downloads/dog.jpg')\n",
    "sample = torch.from_numpy(image).to(device)\n",
    "sample = sample.float().div(255.)\n",
    "sample = sample.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eb553cdf-8cf5-4b16-bd3e-7b13b05a77cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.918933629989624\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    preds = model([sample])\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8431ae5e-08f1-41d6-bd90-919e23c6a2e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[126.4080,  77.7413, 559.8017, 508.7804]]),\n",
       "  'labels': tensor([18]),\n",
       "  'scores': tensor([0.9962]),\n",
       "  'masks': tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]])}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f7183485-8eea-49e4-b73c-3b63948edd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flame.core.model.maskRCNN.rpn import AnchorGenerator\n",
    "from flame.core.model.maskRCNN.faster_rcnn import FasterRCNN\n",
    "from flame.core.model.maskRCNN.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca30ba-2f60-4a52-903a-5a113fd997f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FasterRCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb60da-0cc8-43a1-b300-013a83b9a4df",
   "metadata": {},
   "source": [
    "# Faster RCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "287fb70f-df09-48a6-8620-14fe29419a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "from flame.core.model.maskRCNN.faster_rcnn import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from flame.core.model.maskRCNN.faster_rcnn import fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b8330817-b6ff-4868-9301-f9c3f34e8dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 19327458\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1,\n",
    "    box_score_thresh=0.05,\n",
    "    box_nms_thresh=0.5\n",
    ")\n",
    "model.eval().to(device)\n",
    "print(f'Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e52190df-c634-4b32-b90f-c4278fcfd608",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('/home/phungpx/Downloads/dog.jpg')\n",
    "sample = torch.from_numpy(image).to(device)\n",
    "sample = sample.float().div(255.)\n",
    "sample = sample.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a2ac8c55-1eb1-451b-bc03-14243ae3e0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.329390048980713\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    preds = model([sample])\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "646a2277-c17d-4956-9e17-f0e6cd36c3b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[471.9238,  46.5055, 908.0916, 567.8863],\n",
       "          [468.8514,  40.9219, 813.2957, 569.1475]]),\n",
       "  'labels': tensor([18, 23]),\n",
       "  'scores': tensor([0.9712, 0.2047])}]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a268e66-6d45-4cd3-8b8b-d3e771a4fcb8",
   "metadata": {},
   "source": [
    "# Custome FasterRCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3cd269ac-787b-4385-9ac8-da9d1b6e9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "from flame.core.model.maskRCNN.faster_rcnn import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from flame.core.model.maskRCNN.backbones.mobilenetv3 import MobileNet_V3_Large_Weights\n",
    "\n",
    "# backbone\n",
    "# FPN\n",
    "# Anchor Generator\n",
    "# RoIAlign\n",
    "from torchvision.ops import MultiScaleRoIAlign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "821729c6-fc16-4c35-bbd4-141e548986df",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_backbone_layers = _validate_trainable_layers(True, None, 6, 3)\n",
    "backbone = mobilenet_v3_large(weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)\n",
    "backbone = _mobilenet_extractor(backbone, True, trainable_backbone_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "299bcd0f-7404-40e4-8586-2a88b2de0243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([2, 256, 7, 7])\n",
      "1 torch.Size([2, 256, 7, 7])\n",
      "pool torch.Size([2, 256, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "dummy = torch.FloatTensor(2, 3, 224, 224)\n",
    "output = backbone(dummy)\n",
    "for i, j in output.items():\n",
    "    print(i, j.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "38357c8a-1c97-4c4e-bc06-94813b39d60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = (128, 256, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "910e50c8-4c80-4e40-95f2-45090a7176da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(sizes[0], (list, tuple)):\n",
    "    # TODO change this\n",
    "    sizes = tuple((s,) for s in sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6386e069-d438-4922-852e-e7150cd088a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((128,), (256,), (512,))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e612a037-b417-4b04-9352-e1c28dccb0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor = AnchorGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c250968-ee60-4330-b49d-b22cdad293e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from flame.core.model.maskRCNN.faster_rcnn import FasterRCNN\n",
    "from flame.core.model.maskRCNN.backbones.mobilenetv3 import mobilenet_v3_large, MobileNet_V3_Large_Weights\n",
    "from flame.core.model.maskRCNN.faster_rcnn import fasterrcnn_mobilenet_v3_large_fpn, FasterRCNN_MobileNet_V3_Large_FPN_Weights\n",
    "from flame.core.model.maskRCNN.backbones.backbone_utils import _mobilenet_extractor, _validate_trainable_layers\n",
    "from flame.core.model.maskRCNN.anchor import AnchorGenerator\n",
    "from flame.core.model.maskRCNN.functions._utils import _ovewrite_value_param\n",
    "from flame.core.model.maskRCNN.functions import misc as misc_nn_ops\n",
    "from typing import Optional, Tuple, Any\n",
    "\n",
    "class FasterRCNNMobileNetV3LargeFPN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        weights: Optional[FasterRCNN_MobileNet_V3_Large_FPN_Weights] = None,\n",
    "        weights_backbone: Optional[MobileNet_V3_Large_Weights] = None,\n",
    "        num_classes: int = None,\n",
    "        progress: bool = True,\n",
    "        trainable_backbone_layers: Optional[int] = None,\n",
    "        # Anchors parameters\n",
    "        anchor_sizes: Tuple[Tuple[int]] = ((32, 64, 128, 256, 512,),) * 3,\n",
    "        aspect_ratios: Tuple[Tuple[float]] = ((0.5, 1.0, 2.0),) * 3,\n",
    "        # transform parameters\n",
    "        min_size: int = 320,\n",
    "        max_size: int = 640,\n",
    "        # RPN parameters\n",
    "        rpn_pre_nms_top_n_test: int = 150,\n",
    "        rpn_post_nms_top_n_test: int = 150,\n",
    "        rpn_nms_thresh: float = 0.7,\n",
    "        rpn_score_thresh: float = 0.05,\n",
    "        # Box parameters\n",
    "        box_score_thresh: float = 0.05,\n",
    "        box_nms_thresh: float = 0.5,\n",
    "        **kwargs: Any,\n",
    "    ):\n",
    "        super(FasterRCNNMobileNetV3LargeFPN, self).__init__()\n",
    "        weights = FasterRCNN_MobileNet_V3_Large_FPN_Weights.verify(weights)\n",
    "        weights_backbone = MobileNet_V3_Large_Weights.verify(weights_backbone)\n",
    "\n",
    "        if weights is not None:\n",
    "            weights_backbone = None\n",
    "            num_classes = _ovewrite_value_param(num_classes, len(weights.meta[\"categories\"]))\n",
    "        elif num_classes is None:\n",
    "            num_classes = 91\n",
    "\n",
    "        is_trained = weights is not None or weights_backbone is not None\n",
    "        trainable_backbone_layers = _validate_trainable_layers(is_trained, trainable_backbone_layers, 6, 3)\n",
    "        norm_layer = misc_nn_ops.FrozenBatchNorm2d if is_trained else nn.BatchNorm2d\n",
    "\n",
    "        backbone = mobilenet_v3_large(weights=weights_backbone, progress=progress, norm_layer=norm_layer)\n",
    "        backbone = _mobilenet_extractor(backbone, True, trainable_backbone_layers)\n",
    "\n",
    "        self.model = FasterRCNN(\n",
    "            backbone,\n",
    "            num_classes,\n",
    "            rpn_anchor_generator=AnchorGenerator(anchor_sizes, aspect_ratios),\n",
    "            min_size=min_size,\n",
    "            max_size=max_size,\n",
    "            # RPN parameters\n",
    "            rpn_pre_nms_top_n_test=rpn_pre_nms_top_n_test,\n",
    "            rpn_post_nms_top_n_test=rpn_post_nms_top_n_test,\n",
    "            rpn_nms_thresh=rpn_nms_thresh,\n",
    "            rpn_score_thresh=rpn_score_thresh,\n",
    "            # Box parameters\n",
    "            box_score_thresh=box_score_thresh,\n",
    "            box_nms_thresh=box_nms_thresh,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        if weights is not None:\n",
    "            self.model.load_state_dict(weights.get_state_dict(progress=progress))\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "    def forward(self, x, targets=None):\n",
    "        return self.model(x, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "824e2f58-1db2-4d59-ac98-32fca9ad52d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: 19327458\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "model = FasterRCNNMobileNetV3LargeFPN(weights=FasterRCNN_MobileNet_V3_Large_FPN_Weights.COCO_V1)\n",
    "model.eval().to(device)\n",
    "print(f'Params: {sum(p.numel() for p in model.parameters() if p.requires_grad)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af04c1a5-37bc-49fc-9fc3-11c3c54b82cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "image = cv2.imread('/home/phungpx/Downloads/dog.jpg')\n",
    "sample = torch.from_numpy(image).to(device)\n",
    "sample = sample.float().div(255.)\n",
    "sample = sample.permute(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bba0af5d-2beb-4f1d-af6c-3e6374bd0cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3305342197418213\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t1 = time.time()\n",
    "with torch.no_grad():\n",
    "    preds = model([sample])\n",
    "t2 = time.time()\n",
    "print(t2 - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f1ab0b3-4304-4349-881e-fc336c4605f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'boxes': tensor([[472.7997,  37.1727, 904.9170, 561.7517],\n",
       "          [464.8203,  44.0201, 920.6664, 561.7022],\n",
       "          [475.1435,  46.0952, 888.1178, 562.9543]]),\n",
       "  'labels': tensor([20, 18, 16]),\n",
       "  'scores': tensor([0.8297, 0.4276, 0.0503])}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d6aae9-23b0-472d-b40e-4ebecb1cefd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
